{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Prediction for ecommerce products\n",
    "## Machine Learning Engineer Nanodegree\n",
    "#### MANCEÃ‘IDO AGUSTIN, 02/2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bkgr.jpg\" alt=\"Backgroud\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brand manufacturers should constantly look to evaluate their pricing, learn what works, and make improvements. If the price is too low, you run the risk of not being able to obtain a healthy profit. On the other hand, if the price is too high and you run the risk of not making any sale. **Optimal pricing** is as much art as it is science, and it can mean the difference between an avalanche of orders flowing out or a mountain of excess stock sitting somewhere. Pricing products and services online is one of the most exciting and complex exercises and, as an online business, finding the right formula is one of the most important questions to solve\n",
    "\n",
    "For this project a **dataset of historical sells of an e-commerce site** will be used to develop an algorithm to **predict the optimal** competitive price for a new product to be published. The algorithm needs to determine what is the relevant data in order to get the best prediction for the price taking into account seasonal prices, market fluctuations, economic trends, better sellers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Notes:** \n",
    "* For the complete analysis of the dataset and the feature engineering that is used in this document, please refer to the [Capstone exploration](https://machinelearningmastery.com/persistence-time-series-forecasting-with-python/) file.\n",
    "* For the bechmarks models used as baseline for this project, please refer to the [Capstone benchmarks](https://machinelearningmastery.com/persistence-time-series-forecasting-with-python/) file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "* [Step 0](#step0): Import libraries & Load dataset\n",
    "* [Step 1](#step1): Feature Engineering\n",
    "* [Step 2](#step2): Split data\n",
    "* [Step 3](#step3): Keras model\n",
    "* [Step 4](#step4): Make predictions and test\n",
    "* [Step 5](#step5): K-Fold cross validation\n",
    "* [Step 6](#step6): Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import libraries & Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Import libraries\n",
    "\n",
    "## General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import math\n",
    "from ast import literal_eval\n",
    "\n",
    "## Plot\n",
    "import IPython\n",
    "import keras\n",
    "from IPython.display import display\n",
    "\n",
    "## Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "## Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "## elapsed time\n",
    "t_start = time()\n",
    "def print_elapsed(text=''):\n",
    "    took = (time() - t_start) / 60.0\n",
    "    print('==== \"%s\" elapsed %.3f minutes' % (text, took))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 788642 samples with 8 features each.\n",
      "==== \"dataset loaded\" elapsed 0.014 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>hour</th>\n",
       "      <th>title_len</th>\n",
       "      <th>state_city</th>\n",
       "      <th>seq_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.685848</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>1038</td>\n",
       "      <td>[41, 1, 57, 59, 18, 4, 159, 224]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.685848</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>1038</td>\n",
       "      <td>[41, 1, 57, 59, 18, 4, 159, 224]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.685848</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>1038</td>\n",
       "      <td>[41, 1, 57, 59, 18, 4, 159, 940]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  weekday  day  week  hour  title_len  state_city  \\\n",
       "0  6.685848        4   17    11    16          9        1038   \n",
       "1  6.685848        4   17    11    16          9        1038   \n",
       "2  6.685848        4   17    11    18         10        1038   \n",
       "\n",
       "                          seq_title  \n",
       "0  [41, 1, 57, 59, 18, 4, 159, 224]  \n",
       "1  [41, 1, 57, 59, 18, 4, 159, 224]  \n",
       "2  [41, 1, 57, 59, 18, 4, 159, 940]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "try:\n",
    "    data = pd.read_csv(\"database_eng.csv\")\n",
    "    print(\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    print_elapsed('dataset loaded')\n",
    "except:\n",
    "    print(\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "\n",
    "display(data.head(n = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dataset** to be used on this project has been collected from an Argentine ecommerce site using the official API It consists 795295 records of sells with with 8 features each, all taken between 03/17/2017 and 05/10/2018. The following features are included:\n",
    "* **title**: name of the product provided by the seller.\n",
    "* **sellerid**: unique id for each seller.\n",
    "* **state / city**: seller location.\n",
    "* **pos**: position where the product was found in the search.\n",
    "* **sqty**: number of products selled.\n",
    "* **price**: price of the product.\n",
    "* **created_at**: timestamp for the sell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature engineering** is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Using the results from the **dataset exploration** the features will be modified to increase the predictive power of machine learning algorithm by creating features from raw data that help facilitate the machine learning process.\n",
    "\n",
    "> Dataset is analyzed and modified in the **capstone_exploration** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading array stored as string\n",
    "data['seq_title'] = data['seq_title'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== \"Data split completed\" elapsed 0.370 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitsuga/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "target = data['price']\n",
    "features = data.drop('price', axis = 1)\n",
    "f_train, X_test, t_train, y_test = train_test_split(features, target, random_state=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(f_train, t_train, random_state=123, train_size=0.99)\n",
    "\n",
    "print_elapsed('Data split completed')\n",
    "del target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to keras dict\n",
    "MAX_TITLE = np.max([np.max(data.seq_title.max())])+20\n",
    "MAX_STATE_CITY = np.max([data.state_city.max()])+1\n",
    "MAX_TITLE_SEQ = 15\n",
    "\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'weekday':np.array(dataset.weekday),\n",
    "        'week':np.array(dataset.week),\n",
    "        'day':np.array(dataset.day),\n",
    "        'hour':np.array(dataset.hour),\n",
    "        'title_len':np.array(dataset.title_len),\n",
    "        'state_city':np.array(dataset.state_city),\n",
    "        'seq_title': pad_sequences(dataset.seq_title, maxlen=MAX_TITLE_SEQ)\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_keras_train = get_keras_data(X_train)\n",
    "X_keras_test = get_keras_data(X_test)\n",
    "X_keras_valid = get_keras_data(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "seq_title (InputLayer)          (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "state_city (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 15, 50)       1000950     seq_title[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 20)        36460       state_city[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 13, 16)       2416        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 16)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "weekday (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "week (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "day (InputLayer)                (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hour (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_len (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 41)           0           flatten_1[0][0]                  \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 weekday[0][0]                    \n",
      "                                                                 week[0][0]                       \n",
      "                                                                 day[0][0]                        \n",
      "                                                                 hour[0][0]                       \n",
      "                                                                 title_len[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 41)           164         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          10752       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 131)          0           dropout_2[0][0]                  \n",
      "                                                                 title_len[0][0]                  \n",
      "                                                                 week[0][0]                       \n",
      "                                                                 hour[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 131)          524         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8448        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            65          dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,092,675\n",
      "Trainable params: 1,092,331\n",
      "Non-trainable params: 344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, Activation, concatenate, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    # Stop training when a monitored quantity has stopped improving.\n",
    "    early_stopper = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    # ModelCheckpoint: Save the model after every epoch.\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath,verbose=1, save_best_only=True)\n",
    "    \n",
    "    return [early_stopper, checkpointer]\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    # Inputs (instantiate a Keras tensor)\n",
    "    weekday = Input(shape=[1], name=\"weekday\")\n",
    "    week = Input(shape=[1], name=\"week\")\n",
    "    day = Input(shape=[1], name=\"day\")\n",
    "    hour = Input(shape=[1], name=\"hour\")\n",
    "    title_len = Input(shape=[1], name=\"title_len\")\n",
    "    state_city = Input(shape=[1], name=\"state_city\")\n",
    "    seq_title = Input(shape=[X_keras_train[\"seq_title\"].shape[1]], name=\"seq_title\")\n",
    "    \n",
    "    # Embeddings layers (Turns into dense vectors of fixed size)\n",
    "    emb_state_city = Embedding(MAX_STATE_CITY, 20)(state_city)\n",
    "    emb_seq_title = Embedding(MAX_TITLE, 50)(seq_title)\n",
    " \n",
    "    # Flatten\n",
    "    flat_layer = Flatten() (emb_state_city)\n",
    "    \n",
    "    # CNN layer\n",
    "    cnn_layer = Conv1D(filters=16, kernel_size=3, activation='relu')(emb_seq_title)\n",
    "    cnn_layer = GlobalMaxPooling1D()(cnn_layer)\n",
    "    \n",
    "    # Concatenate\n",
    "    concat = concatenate([\n",
    "        flat_layer\n",
    "        , cnn_layer\n",
    "        , weekday\n",
    "        , week\n",
    "        , day\n",
    "        , hour\n",
    "        , title_len\n",
    "    ])\n",
    "    \n",
    "    # Normalization\n",
    "    normal_1 = BatchNormalization()(concat)\n",
    "    \n",
    "    # Add Densely-connected NN layer + Dropout to prevent overfitting\n",
    "    dense_1 = Dropout(0.6) (Dense(256, activation=\"relu\") (normal_1))\n",
    "    dense = Dropout(0.5) (Dense(128, activation=\"relu\") (dense_1))\n",
    "    \n",
    "    # Concatenate\n",
    "    concat_dense = concatenate([\n",
    "        dense\n",
    "        , title_len\n",
    "        , week\n",
    "        , hour\n",
    "    ])\n",
    "    \n",
    "    # Normalization\n",
    "    normal_2 = BatchNormalization()(concat_dense)\n",
    "    \n",
    "    # Add Densely-connected NN layer + Dropout to prevent overfitting\n",
    "    dense_final = Dropout(0.3) (Dense(64, activation=\"relu\") (normal_2))\n",
    "    \n",
    "    # Set output\n",
    "    output = Dense(1, activation=\"linear\") (dense_final)\n",
    "    # Define model\n",
    "    model = Model([weekday, week, day, hour, title_len, state_city, seq_title], output)\n",
    "    # Compile model\n",
    "    model.compile(loss=\"mean_squared_logarithmic_error\", optimizer=\"adam\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1067pt\" viewBox=\"0.00 0.00 1353.50 1067.00\" width=\"1354pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1063)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1063 1349.5,-1063 1349.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140122743027216 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140122743027216</title>\n",
       "<polygon fill=\"none\" points=\"905,-1022.5 905,-1058.5 1072,-1058.5 1072,-1022.5 905,-1022.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"988.5\" y=\"-1036.8\">seq_title: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140122740382544 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140122740382544</title>\n",
       "<polygon fill=\"none\" points=\"887,-949.5 887,-985.5 1090,-985.5 1090,-949.5 887,-949.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"988.5\" y=\"-963.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 140122743027216&#45;&gt;140122740382544 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140122743027216-&gt;140122740382544</title>\n",
       "<path d=\"M988.5,-1022.4551C988.5,-1014.3828 988.5,-1004.6764 988.5,-995.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"992.0001,-995.5903 988.5,-985.5904 985.0001,-995.5904 992.0001,-995.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122743026896 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140122743026896</title>\n",
       "<polygon fill=\"none\" points=\"300.5,-949.5 300.5,-985.5 476.5,-985.5 476.5,-949.5 300.5,-949.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388.5\" y=\"-963.8\">state_city: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140122743027536 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140122743027536</title>\n",
       "<polygon fill=\"none\" points=\"287,-876.5 287,-912.5 490,-912.5 490,-876.5 287,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388.5\" y=\"-890.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140122743026896&#45;&gt;140122743027536 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140122743026896-&gt;140122743027536</title>\n",
       "<path d=\"M388.5,-949.4551C388.5,-941.3828 388.5,-931.6764 388.5,-922.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"392.0001,-922.5903 388.5,-912.5904 385.0001,-922.5904 392.0001,-922.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122743027280 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140122743027280</title>\n",
       "<polygon fill=\"none\" points=\"913,-876.5 913,-912.5 1064,-912.5 1064,-876.5 913,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"988.5\" y=\"-890.8\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140122740382544&#45;&gt;140122743027280 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140122740382544-&gt;140122743027280</title>\n",
       "<path d=\"M988.5,-949.4551C988.5,-941.3828 988.5,-931.6764 988.5,-922.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"992.0001,-922.5903 988.5,-912.5904 985.0001,-922.5904 992.0001,-922.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122743026384 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140122743026384</title>\n",
       "<polygon fill=\"none\" points=\"318,-803.5 318,-839.5 459,-839.5 459,-803.5 318,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388.5\" y=\"-817.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 140122743027536&#45;&gt;140122743026384 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140122743027536-&gt;140122743026384</title>\n",
       "<path d=\"M388.5,-876.4551C388.5,-868.3828 388.5,-858.6764 388.5,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"392.0001,-849.5903 388.5,-839.5904 385.0001,-849.5904 392.0001,-849.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122743028560 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140122743028560</title>\n",
       "<polygon fill=\"none\" points=\"813.5,-803.5 813.5,-839.5 1163.5,-839.5 1163.5,-803.5 813.5,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"988.5\" y=\"-817.8\">global_max_pooling1d_1: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140122743027280&#45;&gt;140122743028560 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140122743027280-&gt;140122743028560</title>\n",
       "<path d=\"M988.5,-876.4551C988.5,-868.3828 988.5,-858.6764 988.5,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"992.0001,-849.5903 988.5,-839.5904 985.0001,-849.5904 992.0001,-849.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122740584016 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140122740584016</title>\n",
       "<polygon fill=\"none\" points=\"451,-730.5 451,-766.5 672,-766.5 672,-730.5 451,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-744.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140122743026384&#45;&gt;140122740584016 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140122743026384-&gt;140122740584016</title>\n",
       "<path d=\"M431.2641,-803.4551C454.8647,-793.4964 484.36,-781.0504 509.3102,-770.5223\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"510.7757,-773.7028 518.6283,-766.5904 508.0543,-767.2535 510.7757,-773.7028\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122743028560&#45;&gt;140122740584016 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140122743028560-&gt;140122740584016</title>\n",
       "<path d=\"M882.9494,-803.4551C820.475,-792.7744 741.2655,-779.2327 677.1084,-768.2644\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"677.3925,-764.7623 666.9457,-766.527 676.2128,-771.6622 677.3925,-764.7623\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122742972368 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140122742972368</title>\n",
       "<polygon fill=\"none\" points=\"477,-803.5 477,-839.5 646,-839.5 646,-803.5 477,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-817.8\">weekday: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140122742972368&#45;&gt;140122740584016 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140122742972368-&gt;140122740584016</title>\n",
       "<path d=\"M561.5,-803.4551C561.5,-795.3828 561.5,-785.6764 561.5,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-776.5903 561.5,-766.5904 558.0001,-776.5904 565.0001,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122740382928 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140122740382928</title>\n",
       "<polygon fill=\"none\" points=\"0,-803.5 0,-839.5 143,-839.5 143,-803.5 0,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"71.5\" y=\"-817.8\">week: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140122740382928&#45;&gt;140122740584016 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140122740382928-&gt;140122740584016</title>\n",
       "<path d=\"M143.3913,-804.5354C146.1251,-803.9983 148.8351,-803.4841 151.5,-803 248.3153,-785.4122 358.9617,-771.0374 440.8909,-761.4753\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"441.4594,-764.9329 450.9895,-760.304 440.6528,-757.9795 441.4594,-764.9329\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122724963152 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>140122724963152</title>\n",
       "<polygon fill=\"none\" points=\"451,-292.5 451,-328.5 672,-328.5 672,-292.5 451,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-306.8\">concatenate_2: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140122740382928&#45;&gt;140122724963152 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>140122740382928-&gt;140122724963152</title>\n",
       "<path d=\"M130.689,-803.4527C172.1513,-789.26 221.6719,-768.5916 230.5,-748.5\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<path d=\"M230.5,-748.5C256.603,-689.0929 230.5,-667.3889 230.5,-602.5 230.5,-602.5 230.5,-602.5 230.5,-456.5 230.5,-403.3685 375.0755,-356.6953 473.1276,-331.151\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"474.2461,-334.4772 483.0576,-328.5945 472.5008,-327.6983 474.2461,-334.4772\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122740381520 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140122740381520</title>\n",
       "<polygon fill=\"none\" points=\"664,-803.5 664,-839.5 795,-839.5 795,-803.5 664,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"729.5\" y=\"-817.8\">day: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140122740381520&#45;&gt;140122740584016 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140122740381520-&gt;140122740584016</title>\n",
       "<path d=\"M687.9719,-803.4551C665.1543,-793.5403 636.6628,-781.16 612.5019,-770.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"613.699,-767.3656 603.1326,-766.5904 610.9093,-773.7857 613.699,-767.3656\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122740382352 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140122740382352</title>\n",
       "<polygon fill=\"none\" points=\"161,-803.5 161,-839.5 300,-839.5 300,-803.5 161,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230.5\" y=\"-817.8\">hour: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140122740382352&#45;&gt;140122740584016 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140122740382352-&gt;140122740584016</title>\n",
       "<path d=\"M300.2245,-804.8873C303.0184,-804.2455 305.7852,-803.6143 308.5,-803 360.3574,-791.2661 418.331,-778.7352 465.6863,-768.6513\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"466.571,-772.0415 475.6242,-766.5378 465.1149,-765.1946 466.571,-772.0415\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122740382352&#45;&gt;140122724963152 -->\n",
       "<g class=\"edge\" id=\"edge21\">\n",
       "<title>140122740382352-&gt;140122724963152</title>\n",
       "<path d=\"M226.5666,-803.2693C224.167,-787.9756 222.8526,-765.9044 230.5,-748.5\" fill=\"none\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122740383056 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140122740383056</title>\n",
       "<polygon fill=\"none\" points=\"1181.5,-803.5 1181.5,-839.5 1345.5,-839.5 1345.5,-803.5 1181.5,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1263.5\" y=\"-817.8\">title_len: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140122740383056&#45;&gt;140122740584016 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140122740383056-&gt;140122740584016</title>\n",
       "<path d=\"M1181.2323,-804.347C1178.2892,-803.8698 1175.3716,-803.4186 1172.5,-803 1003.2436,-778.3253 805.8693,-763.1599 682.2792,-755.284\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"682.2339,-751.7743 672.0332,-754.637 681.7927,-758.7603 682.2339,-751.7743\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122740383056&#45;&gt;140122724963152 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>140122740383056-&gt;140122724963152</title>\n",
       "<path d=\"M1211.7932,-803.4436C1137.5273,-775.9027 1009.5,-721.7218 1009.5,-675.5 1009.5,-675.5 1009.5,-675.5 1009.5,-456.5 1009.5,-387.5363 815.1648,-346.0823 682.4816,-325.8163\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"682.6511,-322.3025 672.2416,-324.2754 681.6094,-329.2245 682.6511,-322.3025\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122740583888 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140122740583888</title>\n",
       "<polygon fill=\"none\" points=\"395.5,-657.5 395.5,-693.5 727.5,-693.5 727.5,-657.5 395.5,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-671.8\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140122740584016&#45;&gt;140122740583888 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140122740584016-&gt;140122740583888</title>\n",
       "<path d=\"M561.5,-730.4551C561.5,-722.3828 561.5,-712.6764 561.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-703.5903 561.5,-693.5904 558.0001,-703.5904 565.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122738777744 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140122738777744</title>\n",
       "<polygon fill=\"none\" points=\"497.5,-584.5 497.5,-620.5 625.5,-620.5 625.5,-584.5 497.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-598.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140122740583888&#45;&gt;140122738777744 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140122740583888-&gt;140122738777744</title>\n",
       "<path d=\"M561.5,-657.4551C561.5,-649.3828 561.5,-639.6764 561.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-630.5903 561.5,-620.5904 558.0001,-630.5904 565.0001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140123913523088 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>140123913523088</title>\n",
       "<polygon fill=\"none\" points=\"483,-511.5 483,-547.5 640,-547.5 640,-511.5 483,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-525.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 140122738777744&#45;&gt;140123913523088 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140122738777744-&gt;140123913523088</title>\n",
       "<path d=\"M561.5,-584.4551C561.5,-576.3828 561.5,-566.6764 561.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-557.5903 561.5,-547.5904 558.0001,-557.5904 565.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122715866128 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>140122715866128</title>\n",
       "<polygon fill=\"none\" points=\"497.5,-438.5 497.5,-474.5 625.5,-474.5 625.5,-438.5 497.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-452.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140123913523088&#45;&gt;140122715866128 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140123913523088-&gt;140122715866128</title>\n",
       "<path d=\"M561.5,-511.4551C561.5,-503.3828 561.5,-493.6764 561.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-484.5903 561.5,-474.5904 558.0001,-484.5904 565.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122725038288 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>140122725038288</title>\n",
       "<polygon fill=\"none\" points=\"483,-365.5 483,-401.5 640,-401.5 640,-365.5 483,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-379.8\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 140122715866128&#45;&gt;140122725038288 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140122715866128-&gt;140122725038288</title>\n",
       "<path d=\"M561.5,-438.4551C561.5,-430.3828 561.5,-420.6764 561.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-411.5903 561.5,-401.5904 558.0001,-411.5904 565.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122725038288&#45;&gt;140122724963152 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>140122725038288-&gt;140122724963152</title>\n",
       "<path d=\"M561.5,-365.4551C561.5,-357.3828 561.5,-347.6764 561.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-338.5903 561.5,-328.5904 558.0001,-338.5904 565.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122716435984 -->\n",
       "<g class=\"node\" id=\"node20\">\n",
       "<title>140122716435984</title>\n",
       "<polygon fill=\"none\" points=\"395.5,-219.5 395.5,-255.5 727.5,-255.5 727.5,-219.5 395.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-233.8\">batch_normalization_2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140122724963152&#45;&gt;140122716435984 -->\n",
       "<g class=\"edge\" id=\"edge22\">\n",
       "<title>140122724963152-&gt;140122716435984</title>\n",
       "<path d=\"M561.5,-292.4551C561.5,-284.3828 561.5,-274.6764 561.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-265.5903 561.5,-255.5904 558.0001,-265.5904 565.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122716139088 -->\n",
       "<g class=\"node\" id=\"node21\">\n",
       "<title>140122716139088</title>\n",
       "<polygon fill=\"none\" points=\"497.5,-146.5 497.5,-182.5 625.5,-182.5 625.5,-146.5 497.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-160.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140122716435984&#45;&gt;140122716139088 -->\n",
       "<g class=\"edge\" id=\"edge23\">\n",
       "<title>140122716435984-&gt;140122716139088</title>\n",
       "<path d=\"M561.5,-219.4551C561.5,-211.3828 561.5,-201.6764 561.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-192.5903 561.5,-182.5904 558.0001,-192.5904 565.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122716436752 -->\n",
       "<g class=\"node\" id=\"node22\">\n",
       "<title>140122716436752</title>\n",
       "<polygon fill=\"none\" points=\"483,-73.5 483,-109.5 640,-109.5 640,-73.5 483,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-87.8\">dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 140122716139088&#45;&gt;140122716436752 -->\n",
       "<g class=\"edge\" id=\"edge24\">\n",
       "<title>140122716139088-&gt;140122716436752</title>\n",
       "<path d=\"M561.5,-146.4551C561.5,-138.3828 561.5,-128.6764 561.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-119.5903 561.5,-109.5904 558.0001,-119.5904 565.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140122739617104 -->\n",
       "<g class=\"node\" id=\"node23\">\n",
       "<title>140122739617104</title>\n",
       "<polygon fill=\"none\" points=\"497.5,-.5 497.5,-36.5 625.5,-36.5 625.5,-.5 497.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-14.8\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 140122716436752&#45;&gt;140122739617104 -->\n",
       "<g class=\"edge\" id=\"edge25\">\n",
       "<title>140122716436752-&gt;140122739617104</title>\n",
       "<path d=\"M561.5,-73.4551C561.5,-65.3828 561.5,-55.6764 561.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"565.0001,-46.5903 561.5,-36.5904 558.0001,-46.5904 565.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "IPython.display.SVG(keras.utils.vis_utils.model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 585566 samples, validate on 5915 samples\n",
      "Epoch 1/5\n",
      "585566/585566 [==============================] - 183s 313us/step - loss: 0.0519 - val_loss: 0.0091\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00909, saving model to weights.best.hdf5\n",
      "Epoch 2/5\n",
      "585566/585566 [==============================] - 180s 308us/step - loss: 0.0117 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00909 to 0.00548, saving model to weights.best.hdf5\n",
      "Epoch 3/5\n",
      "585566/585566 [==============================] - 181s 309us/step - loss: 0.0081 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00548 to 0.00535, saving model to weights.best.hdf5\n",
      "Epoch 4/5\n",
      "585566/585566 [==============================] - 181s 309us/step - loss: 0.0063 - val_loss: 0.0048\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00535 to 0.00480, saving model to weights.best.hdf5\n",
      "Epoch 5/5\n",
      "585566/585566 [==============================] - 180s 308us/step - loss: 0.0053 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00480 to 0.00429, saving model to weights.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 5\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "history = model.fit(X_keras_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_keras_valid, y_valid)\n",
    "          , callbacks = get_callbacks('weights.best.hdf5')\n",
    "          , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Make predictions and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test, BATCH_SIZE):\n",
    "    # Get predictions\n",
    "    preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "    # Shape normalizaition: y_test, preds\n",
    "    preds = np.asarray(preds, dtype=np.float32)\n",
    "    # Convert back to real values\n",
    "    preds = np.exp(preds)\n",
    "    # Test score\n",
    "    score = mean_squared_log_error(Y_test, preds)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights from checkpoint\n",
    "model.load_weights('weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape normalizaition: y_test\n",
    "y_test = np.asarray(y_test, dtype=np.float32)\n",
    "y_test = np.exp(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSLE: 0.109\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_score = evaluate_model(model, X_keras_test, y_test, BATCH_SIZE)\n",
    "print('Test MSLE: %.3f' % test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot metrics\n",
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['val_loss'])\n",
    "pyplot.xlabel('Epochs')\n",
    "pyplot.ylabel('MSLE score')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folds and datasets\n",
    "k = 10\n",
    "X_train = f_train\n",
    "y_train = t_train\n",
    "folds = list(KFold(n_splits=k, shuffle=True, random_state=1).split(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train on 532332 samples, validate on 59149 samples\n",
      "Epoch 1/5\n",
      "532332/532332 [==============================] - 169s 317us/step - loss: 0.0532 - val_loss: 0.0070\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00704, saving model to model_fold0_weights.h5\n",
      "Epoch 2/5\n",
      "532332/532332 [==============================] - 166s 311us/step - loss: 0.0124 - val_loss: 0.0052\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00704 to 0.00518, saving model to model_fold0_weights.h5\n",
      "Epoch 3/5\n",
      "532332/532332 [==============================] - 166s 312us/step - loss: 0.0084 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00518 to 0.00374, saving model to model_fold0_weights.h5\n",
      "Epoch 4/5\n",
      "532332/532332 [==============================] - 166s 312us/step - loss: 0.0065 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00374 to 0.00338, saving model to model_fold0_weights.h5\n",
      "Epoch 5/5\n",
      "532332/532332 [==============================] - 166s 312us/step - loss: 0.0055 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00338\n",
      "Test MSLE: 0.114\n",
      "Fold 1\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 168s 316us/step - loss: 0.0432 - val_loss: 0.0070\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00702, saving model to model_fold1_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 166s 313us/step - loss: 0.0117 - val_loss: 0.0052\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00702 to 0.00522, saving model to model_fold1_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 166s 313us/step - loss: 0.0083 - val_loss: 0.0044\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00522 to 0.00436, saving model to model_fold1_weights.h5\n",
      "Epoch 4/5\n",
      "532333/532333 [==============================] - 166s 311us/step - loss: 0.0067 - val_loss: 0.0042\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00436 to 0.00422, saving model to model_fold1_weights.h5\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 166s 311us/step - loss: 0.0057 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00422 to 0.00392, saving model to model_fold1_weights.h5\n",
      "Test MSLE: 0.111\n",
      "Fold 2\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 169s 318us/step - loss: 0.0608 - val_loss: 0.0074\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00739, saving model to model_fold2_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 166s 312us/step - loss: 0.0127 - val_loss: 0.0051\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00739 to 0.00512, saving model to model_fold2_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 167s 313us/step - loss: 0.0083 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00512 to 0.00426, saving model to model_fold2_weights.h5\n",
      "Epoch 4/5\n",
      "532333/532333 [==============================] - 167s 313us/step - loss: 0.0063 - val_loss: 0.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00426 to 0.00400, saving model to model_fold2_weights.h5\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 166s 312us/step - loss: 0.0053 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00400 to 0.00388, saving model to model_fold2_weights.h5\n",
      "Test MSLE: 0.115\n",
      "Fold 3\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 170s 320us/step - loss: 0.0579 - val_loss: 0.0077\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00768, saving model to model_fold3_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0123 - val_loss: 0.0046\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00768 to 0.00455, saving model to model_fold3_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0082 - val_loss: 0.0040\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00455 to 0.00402, saving model to model_fold3_weights.h5\n",
      "Epoch 4/5\n",
      "532333/532333 [==============================] - 168s 315us/step - loss: 0.0064 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00402 to 0.00375, saving model to model_fold3_weights.h5\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 167s 315us/step - loss: 0.0053 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00375 to 0.00357, saving model to model_fold3_weights.h5\n",
      "Test MSLE: 0.107\n",
      "Fold 4\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 170s 320us/step - loss: 0.0560 - val_loss: 0.0077\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00773, saving model to model_fold4_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0125 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00773 to 0.00502, saving model to model_fold4_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0086 - val_loss: 0.0044\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00502 to 0.00438, saving model to model_fold4_weights.h5\n",
      "Epoch 4/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0066 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00438 to 0.00387, saving model to model_fold4_weights.h5\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0056 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00387\n",
      "Test MSLE: 0.120\n",
      "Fold 5\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 170s 320us/step - loss: 0.0636 - val_loss: 0.0079\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00790, saving model to model_fold5_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0124 - val_loss: 0.0056\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00790 to 0.00565, saving model to model_fold5_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 168s 315us/step - loss: 0.0083 - val_loss: 0.0046\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00565 to 0.00463, saving model to model_fold5_weights.h5\n",
      "Epoch 4/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0064 - val_loss: 0.0048\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00463\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 167s 314us/step - loss: 0.0054 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00463 to 0.00434, saving model to model_fold5_weights.h5\n",
      "Test MSLE: 0.166\n",
      "Fold 6\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 172s 323us/step - loss: 0.0629 - val_loss: 0.0076\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00763, saving model to model_fold6_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 168s 316us/step - loss: 0.0128 - val_loss: 0.0069\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00763 to 0.00686, saving model to model_fold6_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 169s 317us/step - loss: 0.0085 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00686 to 0.00426, saving model to model_fold6_weights.h5\n",
      "Epoch 4/5\n",
      "532333/532333 [==============================] - 169s 317us/step - loss: 0.0066 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00426 to 0.00387, saving model to model_fold6_weights.h5\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 169s 317us/step - loss: 0.0055 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00387 to 0.00363, saving model to model_fold6_weights.h5\n",
      "Test MSLE: 0.130\n",
      "Fold 7\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 171s 322us/step - loss: 0.0693 - val_loss: 0.0074\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00740, saving model to model_fold7_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 168s 316us/step - loss: 0.0126 - val_loss: 0.0046\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00740 to 0.00459, saving model to model_fold7_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 169s 317us/step - loss: 0.0084 - val_loss: 0.0046\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00459\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532333/532333 [==============================] - 168s 316us/step - loss: 0.0064 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00459 to 0.00367, saving model to model_fold7_weights.h5\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 168s 316us/step - loss: 0.0054 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00367\n",
      "Test MSLE: 0.115\n",
      "Fold 8\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 172s 324us/step - loss: 0.0552 - val_loss: 0.0096\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00960, saving model to model_fold8_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 168s 316us/step - loss: 0.0126 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00960 to 0.00584, saving model to model_fold8_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 168s 316us/step - loss: 0.0086 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00584 to 0.00492, saving model to model_fold8_weights.h5\n",
      "Epoch 4/5\n",
      "532333/532333 [==============================] - 169s 317us/step - loss: 0.0066 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00492 to 0.00435, saving model to model_fold8_weights.h5\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 169s 317us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00435\n",
      "Test MSLE: 0.129\n",
      "Fold 9\n",
      "Train on 532333 samples, validate on 59148 samples\n",
      "Epoch 1/5\n",
      "532333/532333 [==============================] - 173s 325us/step - loss: 0.0672 - val_loss: 0.0079\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00790, saving model to model_fold9_weights.h5\n",
      "Epoch 2/5\n",
      "532333/532333 [==============================] - 169s 318us/step - loss: 0.0128 - val_loss: 0.0051\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00790 to 0.00509, saving model to model_fold9_weights.h5\n",
      "Epoch 3/5\n",
      "532333/532333 [==============================] - 169s 318us/step - loss: 0.0086 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00509 to 0.00429, saving model to model_fold9_weights.h5\n",
      "Epoch 4/5\n",
      "532333/532333 [==============================] - 170s 318us/step - loss: 0.0067 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00429 to 0.00365, saving model to model_fold9_weights.h5\n",
      "Epoch 5/5\n",
      "532333/532333 [==============================] - 169s 318us/step - loss: 0.0055 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00365 to 0.00362, saving model to model_fold9_weights.h5\n",
      "Test MSLE: 0.114\n"
     ]
    }
   ],
   "source": [
    "# Compile, Fit & evaluate models for each fold\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    \n",
    "    print('Fold %i' % j)\n",
    "    X_train_cv, X_valid_cv = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_cv, y_valid_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    name_weights = \"model_fold\" + str(j) + \"_weights.h5\"\n",
    "    callbacks = get_callbacks(name_weights)\n",
    "    model = get_model()\n",
    "    model.fit(get_keras_data(X_train_cv), y_train_cv\n",
    "          , epochs=EPOCHS, batch_size=BATCH_SIZE\n",
    "          , validation_data=(get_keras_data(X_valid_cv), y_valid_cv)\n",
    "          , callbacks = callbacks\n",
    "          , verbose=1)\n",
    "    \n",
    "    test_score = evaluate_model(model, X_keras_test, y_test, BATCH_SIZE)\n",
    "    print('Test MSLE: %.3f' % test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the **exploration process**, a better understanding of the data set was achieved, allowing to create a new set of characteristics from the existing ones, to improve the performance of the algorithms. Also, the number of records was reduced to remove the worst sellers, since this records have no value to predict a competitive price in the market.\n",
    "\n",
    "The **benchmark process** provided a baseline on performance, giving an idea of how well the final model actually performed on the problem. These were the benchmark results:\n",
    "\n",
    "* Benchmark I - **Persistence Model** = 2.220\n",
    "* Benchmark II - **Xgboost model** = 0.244\n",
    "\n",
    "Finally, the trained **neural network model** was tuned after many attempts, resulting in a better test score than the Xgboost model:\n",
    "\n",
    "* Final score - **Keras model** = 0.109\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
